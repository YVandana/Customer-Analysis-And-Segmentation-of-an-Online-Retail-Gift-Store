# -*- coding: utf-8 -*-
"""Customer Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BegVCI6DHNslkErmu6VN_CQ-Y62c0s0M

# CUSTOMER ANALYSIS AND SEGMENTATION USING K-Means

Analysing and Segmenting Customers of an UK Based Online Giftware Store Using Feature Engineering, K Means Clustering and Visualisation
"""

# importing relevant libraries and setting up pre-requisites

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler

# Setting to make numbers easier to read on display
pd.options.display.float_format = '{:20.2f}'.format

# Show all columns on output
pd.set_option('display.max_columns', 999)

"""## DATA EXPLORATION"""

# Data Source https://archive.ics.uci.edu/dataset/502/online+retail+ii

df = pd.read_excel('/content/Data/online_retail_II.xlsx', sheet_name=0)

# check data
df.head(10)

df.info()

"""In the above code we can see that there are null customer id values
The datatypes seem to be workable and not in need of conversion
"""

df.describe()

"""Here the first thing we can notice is how the minimum quantity is -9600 which given the context of the data being from a sales platoform needs to be investigated
Price also seems to have a negative value which is suspicious
And again Customer Id data is missing a considerable amount of values.
"""

# Looking at object data

df.describe(include='O')

"""Invoice is considered a string
28k unique invoices
4632 stock codes
4681 descriptions
It's intersting that the number of unique stock codes and description doesn't align
"""

# Looking at the missing Customer Ids

df[df["Customer ID"].isna()].head(10)

"""Since there is no customer id we should drop this data even for the transactions that look legitmate without negative quantities and the 0 price."""

# Looking at the Negative Quantities

df[df["Quantity"] < 0].head(10)

"""We can see these are legitimate transactions though with negative quantities it has Customer Id attached to it as well
The thing to notice would be the C infront of the invoice which according to the data source UC Irvince's website indicates a Cancellations.
"""

# Exploring invoice data which doesn't have exactly just 6 digits


df["Invoice"] = df["Invoice"].astype("str")
df[df["Invoice"].str.match("^\\d{6}$") == False]

# Checking if C is the only character that appears in the invoice number

df["Invoice"].str.replace("[0-9]", "", regex=True).unique()

"""As seen we see Nothing, C and A.
Let's investigate invoices that have A in their invoice number
"""

# Invoices with A in them

df[df["Invoice"].str.startswith("A")]

"""Just 3 records that seem to indicate bad debt that look like accounting type invoices
It would be best to remove these while cleaning the data
"""

# Exploring the stock code

df["StockCode"] = df["StockCode"].astype("str")
df[(df["StockCode"].str.match("^\\d{5}$") == False) & (df["StockCode"].str.match("^\\d{5}[a-zA-Z]+$") == False)]["StockCode"].unique()

"""The data source indicates that all stock codes are 5 digits however clearly there are other codes."""

# Exploring if the Stock Codes that are not the expected 5 digit code

df[df["StockCode"].str.contains("^DOT")]

# Looking at the data agian

df.head(10)

"""**Notes**

**Stock Code**
StockCode is meant to follow the pattern [0-9]{5} but seems to have legit values for [0-9]{5}[a-zA-Z]+
Also contains other values:

| Code | Description | Action |
| --- | --- | --- |
| DCGS | Looks valid, some quantities are negative though and customer ID is null | Exclude from clustering |
| D | Looks valid, represents discount values | Exclude from clustering |
| DOT | Looks valid, represents postage charges | Exclude from clustering |
| M or m | Looks valid, represents manual transactions | Exclude from clustering |
| C2 | Carriage transaction - not sure what this means | Exclude from clustering |
| C3 | Not sure, only 1 transaction | Exclude |
| BANK CHARGES or B | Bank charges | Exclude from clustering |
| S | Samples sent to customer | Exclude from clustering |
| TESTXXX | Testing data, not valid | Exclude from clustering |
| gift__XXX | Purchases with gift cards, might be interesting for another analysis, but no customer data | Exclude |
| PADS | Looks like a legit stock code for padding | Include |
| SP1002 | Looks like a special request item, only 2 transactions, 3 look legit, 1 has 0 pricing | Exclude for now|
| AMAZONFEE | Looks like fees for Amazon shipping or something | Exclude for now |
| ADJUSTX | Looks like manual account adjustments by admins | Exclude for now |

## DATA CLEANING
"""

# Creating a copy to preserve the orignal data

cleaned_df = df.copy()

# Keeping invoices that are strictly where there are 6 digits only


cleaned_df["Invoice"] = cleaned_df["Invoice"].astype("str")

mask = (
    cleaned_df["Invoice"].str.match("^\\d{6}$") == True
)

cleaned_df = cleaned_df[mask]

cleaned_df

# Cleaning with the previous conclusion we got from examining the stock codes

cleaned_df["StockCode"] = cleaned_df["StockCode"].astype("str")

mask = (
    (cleaned_df["StockCode"].str.match("^\\d{5}$") == True)
    | (cleaned_df["StockCode"].str.match("^\\d{5}[a-zA-Z]+$") == True)
    | (cleaned_df["StockCode"].str.match("^PADS$") == True)
)

cleaned_df = cleaned_df[mask]

cleaned_df

# Removing data rows where the customer id is null

cleaned_df.dropna(subset=["Customer ID"], inplace=True)

# Descibing the cleaned data set

cleaned_df.describe()

# Checking if there are any price values of 0


len(cleaned_df[cleaned_df["Price"] == 0])

# Removing values that are not greater than 0

cleaned_df = cleaned_df[cleaned_df["Price"] > 0.0]

cleaned_df.describe()

cleaned_df["Price"].min()

len(cleaned_df)/len(df)

"""We have dropped about 23% of the original dataset while clearning

## FEATURE ENGINEERING

Featues that will help us more about customers are frequency of purchase, recency of their transaction and how much they have spent (price).
"""

# Creating a new column sales line total to better understand the customer

cleaned_df["SalesLineTotal"] = cleaned_df["Quantity"] * cleaned_df["Price"]

cleaned_df

# Aggregating the data according to customer id, and computing aggregates for recency, frequency and the sales line total (monetary value)

aggregated_df = cleaned_df.groupby(by="Customer ID", as_index=False) \
    .agg(
        MonetaryValue=("SalesLineTotal", "sum"),
        Frequency=("Invoice", "nunique"),
        LastInvoiceDate=("InvoiceDate", "max")
    )

aggregated_df

# Recency based off of the most recent date in the dataframe

max_invoice_date = aggregated_df["LastInvoiceDate"].max()

aggregated_df["Recency"] = (max_invoice_date - aggregated_df["LastInvoiceDate"]).dt.days

aggregated_df

# Visualising the distribution of the featues in the aggregate data frame to discover any outliers
# Using Histograms

plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.hist(aggregated_df['MonetaryValue'], bins=10, color='skyblue', edgecolor='black')
plt.title('Monetary Value Distribution')
plt.xlabel('Monetary Value')
plt.ylabel('Count')

plt.subplot(1, 3, 2)
plt.hist(aggregated_df['Frequency'], bins=10, color='lightgreen', edgecolor='black')
plt.title('Frequency Distribution')
plt.xlabel('Frequency')
plt.ylabel('Count')

plt.subplot(1, 3, 3)
plt.hist(aggregated_df['Recency'], bins=20, color='salmon', edgecolor='black')
plt.title('Recency Distribution')
plt.xlabel('Recency')
plt.ylabel('Count')

plt.tight_layout()
plt.show()

"""The distributions of the monetary and frequency values looks quite skewed with some outliers while the recency values seems to be poisson distribution without as much outliers"""

# Looking to understand the outliers for each feature using boxplots

plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
sns.boxplot(data=aggregated_df['MonetaryValue'], color='skyblue')
plt.title('Monetary Value Boxplot')
plt.xlabel('Monetary Value')

plt.subplot(1, 3, 2)
sns.boxplot(data=aggregated_df['Frequency'], color='lightgreen')
plt.title('Frequency Boxplot')
plt.xlabel('Frequency')

plt.subplot(1, 3, 3)
sns.boxplot(data=aggregated_df['Recency'], color='salmon')
plt.title('Recency Boxplot')
plt.xlabel('Recency')

plt.tight_layout()
plt.show()

"""The Monetary value and Frequency values there are extreme and large amount of outliers
The recency values do have some outliers
"""

# Separating the outliers for Monetary Feature

M_Q1 = aggregated_df["MonetaryValue"].quantile(0.25)
M_Q3 = aggregated_df["MonetaryValue"].quantile(0.75)
M_IQR = M_Q3 - M_Q1

monetary_outliers_df = aggregated_df[(aggregated_df["MonetaryValue"] > (M_Q3 + 1.5 * M_IQR)) | (aggregated_df["MonetaryValue"] < (M_Q1 - 1.5 * M_IQR))].copy()

monetary_outliers_df.describe()

# Separating the outliers for Frequency Feature

F_Q1 = aggregated_df['Frequency'].quantile(0.25)
F_Q3 = aggregated_df['Frequency'].quantile(0.75)
F_IQR = F_Q3 - F_Q1

frequency_outliers_df = aggregated_df[(aggregated_df['Frequency'] > (F_Q3 + 1.5 * F_IQR)) | (aggregated_df['Frequency'] < (F_Q1 - 1.5 * F_IQR))].copy()

frequency_outliers_df.describe()

# Filtering out the outliers out ogf the aggregate data

non_outliers_df = aggregated_df[(~aggregated_df.index.isin(monetary_outliers_df.index)) & (~aggregated_df.index.isin(frequency_outliers_df.index))]

non_outliers_df.describe()

# Visualising the Non Outlier Data Frame on a Box Plot

plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
sns.boxplot(data=non_outliers_df['MonetaryValue'], color='skyblue')
plt.title('Monetary Value Boxplot')
plt.xlabel('Monetary Value')

plt.subplot(1, 3, 2)
sns.boxplot(data=non_outliers_df['Frequency'], color='lightgreen')
plt.title('Frequency Boxplot')
plt.xlabel('Frequency')

plt.subplot(1, 3, 3)
sns.boxplot(data=non_outliers_df['Recency'], color='salmon')
plt.title('Recency Boxplot')
plt.xlabel('Recency')

plt.tight_layout()
plt.show()

"""We can observe that this is a less skewed. There are still outliers but it is better than before"""

# Plotting the Data

fig = plt.figure(figsize=(8, 8))

ax = fig.add_subplot(projection="3d")

scatter = ax.scatter(non_outliers_df["MonetaryValue"], non_outliers_df["Frequency"], non_outliers_df["Recency"])

ax.set_xlabel('Monetary Value')
ax.set_ylabel('Frequency')
ax.set_zlabel('Recency')

ax.set_title('3D Scatter Plot of Customer Data')

plt.show()

"""### Standard Scaling"""

# Using a scalar for regularisation of the values so the K-Means algorithm since it is as distance based algorithms

scaler = StandardScaler()

scaled_data = scaler.fit_transform(non_outliers_df[["MonetaryValue", "Frequency", "Recency"]])

scaled_data

# Creating a data frame with the scaled values

scaled_data_df = pd.DataFrame(scaled_data, index=non_outliers_df.index, columns=("MonetaryValue", "Frequency", "Recency"))

scaled_data_df

# Plotting a 3 D plot of the data again

fig = plt.figure(figsize=(8, 8))

ax = fig.add_subplot(projection="3d")

scatter = ax.scatter(scaled_data_df["MonetaryValue"], scaled_data_df["Frequency"], scaled_data_df["Recency"])

ax.set_xlabel('Monetary Value')
ax.set_ylabel('Frequency')
ax.set_zlabel('Recency')

ax.set_title('3D Scatter Plot of Customer Data')

plt.show()

"""## KMeans CLUSTERING"""

# Applying K-Means to the Scaled Data and Plotting Intertia and Silhouttee Score against the number of Clusters

max_k = 12

inertia = []
silhoutte_scores = []
k_values = range(2, max_k + 1)

for k in k_values:

    kmeans = KMeans(n_clusters=k, random_state=42, max_iter=1000)

    cluster_labels = kmeans.fit_predict(scaled_data_df)

    sil_score = silhouette_score(scaled_data_df, cluster_labels)

    silhoutte_scores.append(sil_score)

    inertia.append(kmeans.inertia_)

plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.plot(k_values, inertia, marker='o')
plt.title('KMeans Inertia for Different Values of k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.xticks(k_values)
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(k_values, silhoutte_scores, marker='o', color='orange')
plt.title('Silhouette Scores for Different Values of k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.xticks(k_values)
plt.grid(True)

plt.tight_layout()
plt.show()

"""From the Intertia it can be gathered that around 4 to 5 clusters might be ideal,
Anaylsing the Silhoutte Score it can be gathered that the between 4 and 5 clusters there seems to be a very small difference between the scores between 4 and 5 with 4 being higher and thus I am choosing to take 4 clusters
"""

# Getting Cluster Labels for the scaled data
# Number of Clusters = 4

kmeans = KMeans(n_clusters=4, random_state=42, max_iter=1000)

cluster_labels = kmeans.fit_predict(scaled_data_df)

cluster_labels

# Adding a Cluster Label Column

non_outliers_df["Cluster"] = cluster_labels

non_outliers_df

# Plotting a Scatter Plot of Customer Data by Cluster

cluster_colors = {0: '#1f77b4',  # Blue
                  1: '#ff7f0e',  # Orange
                  2: '#2ca02c',  # Green
                  3: '#d62728'}  # Red

colors = non_outliers_df['Cluster'].map(cluster_colors)

fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(projection='3d')

scatter = ax.scatter(non_outliers_df['MonetaryValue'],
                     non_outliers_df['Frequency'],
                     non_outliers_df['Recency'],
                     c=colors,  # Use mapped solid colors
                     marker='o')

ax.set_xlabel('Monetary Value')
ax.set_ylabel('Frequency')
ax.set_zlabel('Recency')

ax.set_title('3D Scatter Plot of Customer Data by Cluster')

plt.show()

# Analysing the Clusters

plt.figure(figsize=(12, 18))

plt.subplot(3, 1, 1)
sns.violinplot(x=non_outliers_df['Cluster'], y=non_outliers_df['MonetaryValue'], palette=cluster_colors, hue=non_outliers_df["Cluster"])
sns.violinplot(y=non_outliers_df['MonetaryValue'], color='gray', linewidth=1.0)
plt.title('Monetary Value by Cluster')
plt.ylabel('Monetary Value')

plt.subplot(3, 1, 2)
sns.violinplot(x=non_outliers_df['Cluster'], y=non_outliers_df['Frequency'], palette=cluster_colors, hue=non_outliers_df["Cluster"])
sns.violinplot(y=non_outliers_df['Frequency'], color='gray', linewidth=1.0)
plt.title('Frequency by Cluster')
plt.ylabel('Frequency')


plt.subplot(3, 1, 3)
sns.violinplot(x=non_outliers_df['Cluster'], y=non_outliers_df['Recency'], palette=cluster_colors, hue=non_outliers_df["Cluster"])
sns.violinplot(y=non_outliers_df['Recency'], color='gray', linewidth=1.0)
plt.title('Recency by Cluster')
plt.ylabel('Recency')

plt.tight_layout()
plt.show()

"""### Analysis of Non-Outlier Data

1. Cluster 0 [RETAIN]:  Has Mid to High Monetary and Frequency Values but lower Recency Values. The data seems to be pretty centered thus not very variable. With this rationale this cluster represents High Value Customers who purchase regularly and recently so the focus on a customer-buisness relationship would be to retain them.

2. Cluster 1: Has Low Monetary and Frequency Values but high Recency Values. With this information we can conclude that the cluster represents customers who no longer engage.

3. Cluster 2: Has Low Monetary, Frequency and Recency values. This is the group of customers who have started engaging with the platform.

4. Cluster 3: Has very high Monetary and Frequency values and very low recenc values. These represent valuable customers who regularly engage and spend a lot on the platform.

Cluster 0 (Blue): "Retain"

**Rationale:** This cluster represents high-value customers who purchase regularly, though not always very recently. The focus should be on retention efforts to maintain their loyalty and spending levels.
**Action:** Implement loyalty programs, personalized offers, and regular engagement to ensure they remain active.

Cluster 1 (Orange): "Re-Engage"

**Rationale:** This group includes lower-value, infrequent buyers who haven’t purchased recently. The focus should be on re-engagement to bring them back into active purchasing behavior.
**Action:** Use targeted marketing campaigns, special discounts, or reminders to encourage them to return and purchase again.

Cluster 2 (Green): "Nurture"

**Rationale:** This cluster represents the least active and lowest-value customers, but they have made recent purchases. These customers may be new or need nurturing to increase their engagement and spending.
**Action:** Focus on building relationships, providing excellent customer service, and offering incentives to encourage more frequent purchases.

Cluster 3 (Red): "Reward"

**Rationale:** This cluster includes high-value, very frequent buyers, many of whom are still actively purchasing. They are your most loyal customers, and rewarding their loyalty is key to maintaining their engagement.
**Action:** Implement a robust loyalty program, provide exclusive offers, and recognize their loyalty to keep them engaged and satisfied.

Summary of Cluster Names:
1. Cluster 0 (Blue): "Retain"
2. Cluster 1 (Orange): "Re-Engage"
3. Cluster 2 (Green): "Nurture"
4. Cluster 3 (Red): "Reward"



"""

# We are now going to analyse the Outliers

overlap_indices = monetary_outliers_df.index.intersection(frequency_outliers_df.index)

monetary_only_outliers = monetary_outliers_df.drop(overlap_indices)
frequency_only_outliers = frequency_outliers_df.drop(overlap_indices)
monetary_and_frequency_outliers = monetary_outliers_df.loc[overlap_indices]

monetary_only_outliers["Cluster"] = -1
frequency_only_outliers["Cluster"] = -2
monetary_and_frequency_outliers["Cluster"] = -3

outlier_clusters_df = pd.concat([monetary_only_outliers, frequency_only_outliers, monetary_and_frequency_outliers])

outlier_clusters_df

# Plotting the Outliers on a Violin Plot

cluster_colors = {-1: '#9467bd',
                  -2: '#8c564b',
                  -3: '#e377c2'}

plt.figure(figsize=(12, 18))

plt.subplot(3, 1, 1)
sns.violinplot(x=outlier_clusters_df['Cluster'], y=outlier_clusters_df['MonetaryValue'], palette=cluster_colors, hue=outlier_clusters_df["Cluster"])
sns.violinplot(y=outlier_clusters_df['MonetaryValue'], color='gray', linewidth=1.0)
plt.title('Monetary Value by Cluster')
plt.ylabel('Monetary Value')

plt.subplot(3, 1, 2)
sns.violinplot(x=outlier_clusters_df['Cluster'], y=outlier_clusters_df['Frequency'], palette=cluster_colors, hue=outlier_clusters_df["Cluster"])
sns.violinplot(y=outlier_clusters_df['Frequency'], color='gray', linewidth=1.0)
plt.title('Frequency by Cluster')
plt.ylabel('Frequency')

plt.subplot(3, 1, 3)
sns.violinplot(x=outlier_clusters_df['Cluster'], y=outlier_clusters_df['Recency'], palette=cluster_colors, hue=outlier_clusters_df["Cluster"])
sns.violinplot(y=outlier_clusters_df['Recency'], color='gray', linewidth=1.0)
plt.title('Recency by Cluster')
plt.ylabel('Recency')

plt.tight_layout()
plt.show()

"""### Analysis of the Outlier Data

1. Cluster -1 (Monetary Outliers) PAMPER: Characteristics: High spenders but not necessarily frequent buyers. Their purchases are large but infrequent. Potential Strategy: Focus on maintaining their loyalty with personalized offers or luxury services that cater to their high spending capacity.

2. Cluster -2 (Frequency Outliers) UPSELL: Characteristics: Frequent buyers who spend less per purchase. These customers are consistently engaged but might benefit from upselling opportunities. Potential Strategy: Implement loyalty programs or bundle deals to encourage higher spending per visit, given their frequent engagement.

3. Cluster -3 (Monetary & Frequency Outliers) DELIGHT: Characteristics: The most valuable outliers, with extreme spending and frequent purchases. They are likely your top-tier customers who require special attention. Potential Strategy: Develop VIP programs or exclusive offers to maintain their loyalty and encourage continued engagement.
"""

# Defining the Clusters of both the whole data including the non-outlier and outlier data

cluster_labels = {
    0: "RETAIN",
    1: "RE-ENGAGE",
    2: "NURTURE",
    3: "REWARD",
    -1: "PAMPER",
    -2: "UPSELL",
    -3: "DELIGHT"
}

# Combining the Non-Outlier and Outlier Data with their Cluster Labels into a new Data Frame

full_clustering_df = pd.concat([non_outliers_df, outlier_clusters_df])

full_clustering_df

# Mapping the labels for the Clusters

full_clustering_df["ClusterLabel"] = full_clustering_df["Cluster"].map(cluster_labels)

full_clustering_df

"""## VISUALISATION"""

# Visualising the Full Clustering Data

cluster_counts = full_clustering_df['ClusterLabel'].value_counts()
full_clustering_df["MonetaryValue per 100 pounds"] = full_clustering_df["MonetaryValue"] / 100.00
feature_means = full_clustering_df.groupby('ClusterLabel')[['Recency', 'Frequency', 'MonetaryValue per 100 pounds']].mean()

fig, ax1 = plt.subplots(figsize=(12, 8))

sns.barplot(x=cluster_counts.index, y=cluster_counts.values, ax=ax1, palette='viridis', hue=cluster_counts.index)
ax1.set_ylabel('Number of Customers', color='b')
ax1.set_title('Cluster Distribution with Average Feature Values')

ax2 = ax1.twinx()

sns.lineplot(data=feature_means, ax=ax2, palette='Set2', marker='o')
ax2.set_ylabel('Average Value', color='g')

plt.show()